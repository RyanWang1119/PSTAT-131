---
title: "Homework 5"
author: "PSTAT 131/231"
output:
    html_document:
      toc: true
      toc_float: true
      code_folding: show
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE,
                      warning = FALSE)
library(tidyverse)
library(tidymodels)
library(janitor)
library(forcats)
library(corrplot)
library(tinytex)
library(ISLR)
library(ggplot2)
library(corrplot)
library(ggthemes)
library(yardstick)
library(dplyr)
library(magrittr)
library(corrr)
library(discrim)
library(poissonreg)
library(klaR)
library(glmnet)
library(rpart.plot)
library(randomForest)
library(ranger)
library(vip)
library(xgboost)
```

## Homework 5

For this assignment, we will be working with the file `"pokemon.csv"`, found in `/data`. The file is from Kaggle: <https://www.kaggle.com/abcsds/pokemon>.

The [Pokémon](https://www.pokemon.com/us/) franchise encompasses video games, TV shows, movies, books, and a card game. This data set was drawn from the video game series and contains statistics about 721 Pokémon, or "pocket monsters." In Pokémon games, the user plays as a trainer who collects, trades, and battles Pokémon to (a) collect all the Pokémon and (b) become the champion Pokémon trainer.

Each Pokémon has a [primary type](https://bulbapedia.bulbagarden.net/wiki/Type) (some even have secondary types). Based on their type, a Pokémon is strong against some types, and vulnerable to others. (Think rock, paper, scissors.) A Fire-type Pokémon, for example, is vulnerable to Water-type Pokémon, but strong against Grass-type.

![Fig 1. Vulpix, a Fire-type fox Pokémon from Generation 1 (also my favorite Pokémon!) ](images/vulpix.png){width="196"}

The goal of this assignment is to build a statistical learning model that can predict the **primary type** of a Pokémon based on its generation, legendary status, and six battle statistics. *This is an example of a **classification problem**, but these models can also be used for **regression problems***.

Read in the file and familiarize yourself with the variables using `pokemon_codebook.txt`.

## It is one of my free late submission.

### Exercise 1

Install and load the `janitor` package. Use its `clean_names()` function on the Pokémon data, and save the results to work with for the rest of the assignment. What happened to the data? Why do you think `clean_names()` is useful?

```{r}
pokemon <- read_csv("data/Pokemon.csv") %>% 
  clean_names()
```
clean_names() transform the column names into the "snake case". For instance, the original column names of this dataset contain "Type 1" and "Sp. Atk" that are inconsistent and hard to read. 


### Exercise 2

Using the entire data set, create a bar chart of the outcome variable, `type_1`.

How many classes of the outcome are there? Are there any Pokémon types with very few Pokémon? If so, which ones?

For this assignment, we'll handle the rarer classes by grouping them, or "lumping them," together into an 'other' category. [Using the `forcats` package](https://forcats.tidyverse.org/), determine how to do this, and **lump all the other levels together except for the top 6 most frequent** (which are Bug, Fire, Grass, Normal, Water, and Psychic).

Convert `type_1` and `legendary` to factors.

```{r}
d <- ggplot(pokemon,
       aes(x=reorder(type_1,type_1,
                     function(x)+length(x)))) +
       geom_bar()
d + geom_bar() + coord_flip() + labs(x = 'Type 1')

pokemon$type_1 <- as_factor(pokemon$type_1)
pokemon$legendary <- as_factor(pokemon$legendary)
pokemon$generation <- as_factor(pokemon$generation)
nlevels(pokemon$type_1)
```

```{r}
pokemon$class <- fct_lump_n(
  pokemon$type_1,
  n=6,
  w = NULL,
  other_level = "Other",
  ties.method = c("min", "average", "first", "last", "random", "max")
)

d2 <- ggplot(pokemon,
       aes(x=reorder(class,class,
                     function(x)+length(x)))) +
       geom_bar()
d2 + geom_bar() + coord_flip() + labs(x = 'Class')

levels(pokemon$class)
```

There are eighteen classes of the outcome. The types with few Pokémon are Flying and Fairy.

### Exercise 3

Perform an initial split of the data. Stratify by the outcome variable. You can choose a proportion to use. Verify that your training and test sets have the desired number of observations.

Next, use *v*-fold cross-validation on the training set. Use 5 folds. Stratify the folds by `type_1` as well. *Hint: Look for a `strata` argument.*

Why do you think doing stratified sampling for cross-validation is useful?

```{r}
set.seed(34)
pokemon_split <- initial_split(pokemon, prop = 0.7, strata = class)
pokemon_train <- training(pokemon_split)
pokemon_test <- testing(pokemon_split)

nrow(pokemon_train)/nrow(pokemon)  # 0.6975

pokemon_folds <- vfold_cv(data = pokemon_train, v = 5, strata = class)
```

Stratified sampling for cross-validation is useful because it ensures that the proportion of each class remains relatively the same across the folds. 

### Exercise 4

Create a correlation matrix of the training set, using the `corrplot` package. *Note: You can choose how to handle the categorical variables for this plot; justify your decision(s).*

What relationships, if any, do you notice?
```{r}
nums_col <- unlist(lapply(pokemon_train, is.numeric), use.names = FALSE)  
nums <- pokemon_train[ , nums_col]
corrplot(cor(nums), method = 'number', type = 'lower') 
```

I drop the categorical variables for this plot because corrplot() function cannot work with the categorical variables.
Firstly, the generation of the Pokémon hardly correlated with any of the Pokémon's statistics. Perhaps it is because generation is more like a categorical variable than a numeric variable.
There exists no negative correlation between all variables. For instance, a Pokémon with higher hp is normally better in attacking. 
The "Total" variable is more correlated with "attack", "special attack", "special defense" with correlation coefficient around 0.75. So, a Pokémon with higher statistics in these variables might be better.


### Exercise 5

Set up a recipe to predict `type_1` with `legendary`, `generation`, `sp_atk`, `attack`, `speed`, `defense`, `hp`, and `sp_def`.

-   Dummy-code `legendary` and `generation`;

-   Center and scale all predictors.
```{r}
pokemon_train$generation <- as_factor(pokemon_train$generation)

pokemon_recipe <- recipe(class ~ legendary + generation + 
                           sp_atk + attack + speed + defense + 
                           hp + sp_def, data = pokemon_train) %>% 
  step_dummy(all_nominal_predictors()) %>% 
  step_normalize(all_predictors())
```

### Exercise 6

We'll be fitting and tuning an elastic net, tuning `penalty` and `mixture` (use `multinom_reg()` with the `glmnet` engine).

Set up this model and workflow. Create a regular grid for `penalty` and `mixture` with 10 levels each; `mixture` should range from 0 to 1. For this assignment, let `penalty` range from 0.01 to 3 (this is on the `identity_trans()` scale; note that you'll need to specify these values in base 10 otherwise).

```{r}
elastic_net_mod <- multinom_reg(penalty = tune(), 
                                 mixture = tune()) %>% 
  set_mode("classification") %>% 
  set_engine("glmnet")

en_workflow <- workflow() %>% 
  add_recipe(pokemon_recipe) %>% 
  add_model(elastic_net_mod)

en_grid <- grid_regular(penalty(range = c(0.01, 3),
                                     trans = identity_trans()),
                        mixture(range = c(0, 1)),
                             levels = 10)
```

### Exercise 7

Now set up a random forest model and workflow. Use the `ranger` engine and set `importance = "impurity"`; we'll be tuning `mtry`, `trees`, and `min_n`. Using the documentation for `rand_forest()`, explain in your own words what each of these hyperparameters represent.

Create a regular grid with 8 levels each. You can choose plausible ranges for each hyperparameter. Note that `mtry` should not be smaller than 1 or larger than 8. **Explain why neither of those values would make sense.**

What type of model does `mtry = 8` represent?
```{r}
forest_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification") %>% 
  set_args(mtry = tune(),
           trees = tune(),
           min_n = tune())

forest_wf <- workflow() %>% 
  add_model(forest_spec) %>% 
  add_recipe(pokemon_recipe)

rf_grid <- grid_regular(mtry(range = c(1, 6)), 
                        trees(range = c(200, 600)),
                        min_n(range = c(10, 20)),
                        levels = 8)
```

mtry represents the number of candidate predictors that are taken as a random subset from the whole group of the predictors at one specific step during decision-making. The tree can only access to these candidate predictors when making decision. This is what makes random forest different from bagging.

trees represents number trees that are fitted. The ultimate result will be the mean of the results of these individual trees.

min_n represents the least number of observations that are in one tree leaf.

mtry cannot be less than 1 because if so we do not have any predictor that can be used to make decision. It cannot be greater than 8 because we have 8 predictors in total.


### Exercise 8

Fit all models to your folded data using `tune_grid()`.

**Note: Tuning your random forest model will take a few minutes to run, anywhere from 5 minutes to 15 minutes and up. Consider running your models outside of the .Rmd, storing the results, and loading them in your .Rmd to minimize time to knit. We'll go over how to do this in lecture.**

Use `autoplot()` on the results. What do you notice? Do larger or smaller values of `penalty` and `mixture` produce better ROC AUC? What about values of `min_n`, `trees`, and `mtry`?

What elastic net model and what random forest model perform the best on your folded data? (What specific values of the hyperparameters resulted in the optimal ROC AUC?)

```{r, eval=FALSE}
tune_en <- tune_grid(
  en_workflow,
  resamples = pokemon_folds, 
  grid = en_grid)  # 

save(tune_en, file = "tuning-result_en.rda")
```

```{r}
set.seed(37)
load("tuning-result_en.rda")
autoplot(tune_en) + theme_minimal() + labs(title = "elastic net")
show_best(tune_en, n = 1, metric = "roc_auc")
```

It seems that penalty of 0.01 and mixture of $\frac{5}{9}$ produce the best ROC AUC. As the penalty increases, the model gets worse in terms of ROC AUC, and when penalty is not 0.01, as the mixture increase, ROC AUC also decreases.

```{r, eval=FALSE}
tune_random_f <- tune_grid(
  forest_wf,
  resamples = pokemon_folds, 
  grid = rf_grid)

save(tune_random_f, file = "tuning-result_rf.rda")
```

```{r}
set.seed(37)
load("tuning-result_rf.rda")
autoplot(tune_random_f) + theme_minimal() + labs(title = "random forest")
show_best(tune_random_f, n = 1, metric = "roc_auc")
```

min_n and trees seems not to have much of an effect on the accuracy and ROC AUC. Models with one randomly selected predictor have lower ROC AUC than the models with more than one randomly selected predictors and among the models that have more than one randomly selected predictors, those with four predictors seems to have slightly higher ROC AUC.

The elastic net model with penalty of 0.01 and mixture 0.556 is the best; its ROC AUC is 0.673.

The random forest model with mtry of 3, trees of 542, adn min_n of 14 is the best; its ROC AUC is 0.708.

### Exercise 9

Select your optimal [**random forest model**]{.underline}in terms of `roc_auc`. Then fit that model to your training set and evaluate its performance on the testing set.

Using the **training** set:

-   Create a variable importance plot, using `vip()`. *Note that you'll still need to have set `importance = "impurity"` when fitting the model to your entire training set in order for this to work.*

    -   What variables were most useful? Which were least useful? Are these results what you expected, or not?

```{r}
best_model <- select_best(tune_random_f, metric = "roc_auc")

rf_final <- finalize_workflow(forest_wf, best_model)

rf_final_fit <- fit(rf_final, data = pokemon_train)


rf_final_fit %>% extract_fit_parsnip() %>% 
  vip()
```

"Special attack" and "speed" are the most useful. The "generation" is the least useful. The "legendary" even does not appear in this plot, which means that it is not selected as the predictor at every step of decision-making of the tree, which also means that it is not useful. This is what I am not expected, but it is probably because that each type has similar number of legendary pokemon so that telling one is legendary does not help to predict the type.

Using the testing set:

-   Create plots of the different ROC curves, one per level of the outcome variable;

-   Make a heat map of the confusion matrix.

```{r}
final_model_test <- augment(rf_final_fit, 
                               pokemon_test) %>% 
  dplyr::select(class, starts_with(".pred"))


roc_curve(final_model_test, truth = class, .pred_Grass:.pred_Other) %>% 
  autoplot()

conf_mat(final_model_test, truth = class, 
         .pred_class) %>% 
  autoplot(type = "heatmap")
```


### Exercise 10

How did your best random forest model do on the testing set?

Which Pokemon types is the model best at predicting, and which is it worst at? (Do you have any ideas why this might be?)

```{r}
set.seed(37)
roc_auc(final_model_test, truth = class, .pred_Grass:.pred_Other)
```

The roc_auc on the test data set is 0.716.

The model is most accurate at predicting "Other", and worse at predicting "Grass", "Fire", "Water", "Bug", "Normal", and "Psychic". I think the reason is that the data is imbalanced after we collapse many type into the type "Other", so the model is biased.

## For 231 Students

### Exercise 11

In the 2020-2021 season, Stephen Curry, an NBA basketball player, made 337 out of 801 three point shot attempts (42.1%). Use bootstrap resampling on a sequence of 337 1's (makes) and 464 0's (misses). For each bootstrap sample, compute and save the sample mean (e.g. bootstrap FG% for the player). Use 1000 bootstrap samples to plot a histogram of those values. Compute the 99% bootstrap confidence interval for Stephen Curry's "true" end-of-season FG% using the quantile function in R. Print the endpoints of this interval.

```{r}
curry <- c(rep(1, 337), rep(0, 464))
```

*There may be multiple ways to solve this problem. One is to use a loop to generate the bootstrap samples:*

```{r}
set.seed(38)
bootstrap <- function(n){
sample_mean = list()
for (i in 1:n){
boot = sample(curry, length(curry), replace = T)
sample_mean = append(sample_mean, sum(boot)/length(boot))
}
return (unlist(sample_mean))
}

result <- tibble(sample_mean = bootstrap(1000))
hist(result$sample_mean, main = "Bootstrap FG% for Curry")
quantile(result$sample_mean, c(0.005, 0.995))
```

The histogram is centered around 0.42, which is the actual mean.
The 99% confidence interval is (0.3745, 0.4657).

### Exercise 12

Using the `abalone.txt` data from previous assignments, fit and tune a **random forest** model to predict `age`. Use stratified cross-validation and select ranges for `mtry`, `min_n`, and `trees`. Present your results. What was your final chosen model's **RMSE** on your testing set?

```{r}
# load and prepare data
abalone <- read.csv("data/abalone.csv")
abalone["age"] <- abalone["rings"]+1.5

set.seed(315)
abalone_split <- initial_split(abalone,prop=0.70,strata = age)
ab_train <- training(abalone_split)
ab_test <- testing(abalone_split)

ab_folds <- vfold_cv(ab_train, v = 5, strata = age)
```

```{r}
# recipe
abtrain_no_rings <- ab_train %>% dplyr::select(-rings)

ab_recipe <- recipe(age ~ ., data = abtrain_no_rings) %>%
step_dummy(all_nominal_predictors()) %>%
step_interact(terms= ~ starts_with("type"):shucked_weight+
longest_shell:diameter+
shucked_weight:shell_weight) %>%
step_normalize(all_predictors())
```

```{r}
# workflow
abalone_rf <- rand_forest(mtry = tune(), trees = tune(), min_n = tune()) %>%
set_engine("ranger", importance = "impurity") %>%
set_mode("regression")

abalone_wkflow <- workflow() %>%
add_recipe(ab_recipe) %>%
add_model(abalone_rf)

abalone_grid <- grid_regular(
mtry(range = c(2,9)),
trees(range = c(100,400)),
min_n(range = c(10,20)),
levels = 8
)

```

```{r, eval=FALSE}
abalone_tune <- tune_grid(
abalone_wkflow,
resamples = ab_folds,
grid = abalone_grid)
save(abalone_tune, file = "tuning-result_ab_rf.rda")
```

```{r}
load("tuning-result_ab_rf.rda")
autoplot(abalone_tune)
```

```{r}
show_best(abalone_tune, metric = "rmse")
show_best(abalone_tune, metric = "rsq")
```

The model with mty of 5, number of trees of 400, and minimum nodes of 20 is the best.

```{r}
set.seed(37)
abalone_final_wf <- finalize_workflow(abalone_wkflow,select_best(abalone_tune))
abalone_final_fit <- fit(abalone_final_wf, ab_train)

augment(abalone_final_fit, new_data = ab_test) %>%
rmse(truth = age, estimate = .pred)

augment(abalone_final_fit, new_data = ab_test) %>%
rsq(truth = age, estimate = .pred)
```

The model’s RMSE on the testing set is 2.07, the $R^2$ on the testing set is 0.6. The model performs better on the testing set than it does on the cross-validation.
